{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow import keras\n",
    "import skimage\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/mnist_png/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_vect(label, angle):\n",
    "    label = int(label)\n",
    "    angle = int(angle)\n",
    "    position = (4 - angle)%4\n",
    "    vector = []\n",
    "    for i in range(4):\n",
    "        for j in range(10):\n",
    "            if i == position and j == label:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaddata\n",
    "def loaddata(s):\n",
    "    filelist = glob.glob(directory + s + '/*.png')\n",
    "    y_temp = []\n",
    "    x_temp = []\n",
    "    print (directory + s + '/*.png')\n",
    "    for fname in filelist:\n",
    "        short_name = fname.split('/')[-1]\n",
    "        label, angle, name = short_name.split('_')\n",
    "        img = mpimg.imread(fname)\n",
    "        x_temp.append(img)\n",
    "        vector = str_to_vect(label, angle)\n",
    "        y_temp.append(vector)\n",
    "\n",
    "    x = np.asarray(x_temp)\n",
    "    y = np.asarray(y_temp)\n",
    "    print (s + ' import done. the shapes of x and y are:', x.shape, y.shape)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = loaddata('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = loaddata('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(test_x[5])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y))    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "    \n",
    "    g1_input_layer = tf.image.rot90(input_layer)\n",
    "    g2_input_layer = tf.image.rot90(input_layer, k=2)\n",
    "    g3_input_layer = tf.image.rot90(input_layer, k=3)\n",
    "    # Convolutional Layer #1\n",
    "    conv1_0 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      reuse = None,\n",
    "      name = 'conv1')\n",
    "    \n",
    "    conv1_1 = tf.layers.conv2d(\n",
    "      inputs=g1_input_layer,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      reuse = True,\n",
    "      name = 'conv1')    \n",
    "\n",
    "    conv1_2 = tf.layers.conv2d(\n",
    "      inputs=g2_input_layer,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      reuse = True,\n",
    "      name = 'conv1')  \n",
    "\n",
    "    conv1_3 = tf.layers.conv2d(\n",
    "      inputs=g3_input_layer,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      reuse = True,\n",
    "      name = 'conv1') \n",
    "    \n",
    "    conv1    = tf.concat([conv1_0, conv1_1, conv1_2, conv1_3], axis = 3)\n",
    "    g1_conv1 = tf.concat([conv1_1, conv1_2, conv1_3, conv1_0], axis = 3)\n",
    "    g2_conv1 = tf.concat([conv1_2, conv1_3, conv1_0, conv1_1], axis = 3)\n",
    "    g3_conv1 = tf.concat([conv1_3, conv1_0, conv1_1, conv1_2], axis = 3)\n",
    "    \n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2_0 = tf.layers.conv2d(\n",
    "      inputs=conv1,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = None,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv2')\n",
    "    \n",
    "    conv2_1 = tf.layers.conv2d(\n",
    "      inputs=g1_conv1,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = True,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv2')\n",
    "\n",
    "    conv2_2 = tf.layers.conv2d(\n",
    "      inputs=g2_conv1,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = True,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv2')\n",
    "\n",
    "    conv2_3 = tf.layers.conv2d(\n",
    "      inputs=g3_conv1,\n",
    "      filters=80,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = True,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv2')\n",
    "    \n",
    "    conv2    = tf.concat([conv2_0, conv2_1, conv2_2, conv2_3], axis = 3)\n",
    "    g1_conv2 = tf.concat([conv2_1, conv2_2, conv2_3, conv2_0], axis = 3)\n",
    "    g2_conv2 = tf.concat([conv2_2, conv2_3, conv2_0, conv2_1], axis = 3)\n",
    "    g3_conv2 = tf.concat([conv2_3, conv2_0, conv2_1, conv2_2], axis = 3)\n",
    "\n",
    "    conv3_0 = tf.layers.conv2d(\n",
    "      inputs=conv2,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = None,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv3')\n",
    "\n",
    "    conv3_1 = tf.layers.conv2d(\n",
    "      inputs=g1_conv2,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = True,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv3')\n",
    "\n",
    "    conv3_2 = tf.layers.conv2d(\n",
    "      inputs=g2_conv2,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = True,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv3')\n",
    "\n",
    "    conv3_3 = tf.layers.conv2d(\n",
    "      inputs=g3_conv2,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      reuse = True,\n",
    "      activation=tf.nn.relu,\n",
    "      name = 'conv3')\n",
    "    \n",
    "    conv3    = tf.concat([conv3_0, conv3_1, conv3_2, conv3_3], axis = 3)\n",
    "    g1_conv3 = tf.concat([conv3_1, conv3_2, conv3_3, conv3_0], axis = 3)\n",
    "    g2_conv3 = tf.concat([conv3_2, conv3_3, conv3_0, conv3_1], axis = 3)\n",
    "    g3_conv3 = tf.concat([conv3_3, conv3_0, conv3_1, conv3_2], axis = 3)\n",
    "    \n",
    "    pool4_0 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[4, 4], strides=4)\n",
    "    pool4_1 = tf.layers.max_pooling2d(inputs=g1_conv3, pool_size=[4, 4], strides=4)\n",
    "    pool4_2 = tf.layers.max_pooling2d(inputs=g2_conv3, pool_size=[4, 4], strides=4)\n",
    "    pool4_3 = tf.layers.max_pooling2d(inputs=g3_conv3, pool_size=[4, 4], strides=4)\n",
    "    \n",
    "    pool4_0_flat = tf.reshape(pool4_0, [-1, 7 * 7 * 128])\n",
    "    pool4_1_flat = tf.reshape(pool4_1, [-1, 7 * 7 * 128])\n",
    "    pool4_2_flat = tf.reshape(pool4_2, [-1, 7 * 7 * 128])\n",
    "    pool4_3_flat = tf.reshape(pool4_3, [-1, 7 * 7 * 128])\n",
    "        \n",
    "    dense_0 = tf.layers.dense(inputs=pool4_0_flat, \n",
    "                              units=1024, \n",
    "                              activation=tf.nn.relu,\n",
    "                              reuse = None,\n",
    "                              name = 'dense')\n",
    "\n",
    "    dense_1 = tf.layers.dense(inputs=pool4_1_flat, \n",
    "                              units=1024, \n",
    "                              activation=tf.nn.relu,\n",
    "                              reuse = True,\n",
    "                              name = 'dense')\n",
    "\n",
    "    dense_2 = tf.layers.dense(inputs=pool4_2_flat, \n",
    "                              units=1024, \n",
    "                              activation=tf.nn.relu,\n",
    "                              reuse = True,\n",
    "                              name = 'dense')\n",
    "    \n",
    "    dense_3 = tf.layers.dense(inputs=pool4_3_flat, \n",
    "                              units=1024, \n",
    "                              activation=tf.nn.relu,\n",
    "                              reuse = True,\n",
    "                              name = 'dense')\n",
    "\n",
    "    # Logits Layer\n",
    "    logits_0 = tf.layers.dense(inputs=dense_0,\n",
    "                               units=10,\n",
    "                               reuse = None,\n",
    "                               name = 'logi'\n",
    "                              )\n",
    "\n",
    "    logits_1 = tf.layers.dense(inputs=dense_1, \n",
    "                               units=10,\n",
    "                               reuse = True,\n",
    "                               name = 'logi'\n",
    "                              )\n",
    "\n",
    "    \n",
    "    logits_2 = tf.layers.dense(inputs=dense_2, \n",
    "                               units=10,\n",
    "                               reuse = True,\n",
    "                               name = 'logi'\n",
    "                              )\n",
    "    \n",
    "    logits_3 = tf.layers.dense(inputs=dense_3, \n",
    "                               units=10,\n",
    "                               reuse = True,\n",
    "                               name = 'logi'\n",
    "                              )\n",
    "    \n",
    "    final = tf.concat([logits_0, logits_1, logits_2, logits_3], axis = 1)\n",
    "    \n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=final, axis= 1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(final, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = compute_cost(final, labels)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=tf.argmax(input=labels, axis= -1), predictions=predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_x / 255.0\n",
    "eval_data = test_x / 255.0\n",
    "\n",
    "train_labels = train_y.astype(np.int32)  # not required\n",
    "eval_labels = test_y.astype(np.int32)  # not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data.shape, eval_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=200,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "# train one step and display the probabilties\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier.train(input_fn=train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we test equivariancy on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_x, samples_y = loaddata('samples')\n",
    "samples_x = samples_x/np.float32(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(samples_x[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(samples_x[1])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(samples_x[2])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(samples_x[3])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": samples_x},\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mnist_classifier.predict(input_fn = pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(c):\n",
    "    return c%10, (4 -(c-c%10)/10)%4 * 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in predictions:\n",
    "    print (decode(prediction['classes']), '\\n', prediction['probabilities'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
